{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724215781.156371  409748 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "for key in st.secrets.keys():\n",
    "    os.environ[key] = st.secrets[key]\n",
    "\n",
    "from load_llm import openai_llm, groq_llm, llm, gemini_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# llm = gemini_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a **knowledge graph** specifically designed for **natural language processing (NLP)**. It's a powerful tool that helps computers understand and reason about language in a more human-like way. \n",
      "\n",
      "Here's a breakdown of its key features and how it works:\n",
      "\n",
      "**What it is:**\n",
      "\n",
      "* **A graph database:** LangGraph stores information in a network of interconnected nodes (entities) and edges (relationships). This structure allows for efficient querying and reasoning about complex relationships between words, concepts, and other linguistic elements.\n",
      "* **Knowledge-based:** It contains a vast amount of pre-existing knowledge about language, including:\n",
      "    * **Lexical information:** Word meanings, synonyms, antonyms, etc.\n",
      "    * **Syntactic information:** Grammar rules, part-of-speech tags, etc.\n",
      "    * **Semantic information:** Relationships between concepts, hierarchies, etc.\n",
      "    * **World knowledge:** General facts about the world, common sense reasoning, etc.\n",
      "* **Extensible:** LangGraph can be easily extended with new knowledge and data, allowing it to adapt to specific domains and tasks.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. **Data ingestion:** LangGraph can ingest various types of linguistic data, including text corpora, dictionaries, ontologies, and knowledge bases.\n",
      "2. **Knowledge extraction:** It uses NLP techniques to extract relationships and knowledge from the ingested data.\n",
      "3. **Graph construction:** The extracted knowledge is then organized into a graph structure, connecting entities and relationships.\n",
      "4. **Querying and reasoning:** Users can query the graph to retrieve information, perform logical inferences, and answer complex questions about language.\n",
      "\n",
      "**Benefits of using LangGraph:**\n",
      "\n",
      "* **Improved NLP performance:** LangGraph provides a richer understanding of language, leading to better results in tasks like text classification, machine translation, and question answering.\n",
      "* **Enhanced reasoning capabilities:** It enables computers to reason about language in a more sophisticated way, making them better at understanding complex sentences and resolving ambiguities.\n",
      "* **Scalability and flexibility:** LangGraph can handle large amounts of data and adapt to different NLP tasks.\n",
      "\n",
      "**Applications of LangGraph:**\n",
      "\n",
      "* **Semantic search:** Finding relevant information based on the meaning of words and concepts.\n",
      "* **Text summarization:** Generating concise summaries of long texts by identifying key concepts and relationships.\n",
      "* **Machine translation:** Improving translation quality by leveraging knowledge about language and the world.\n",
      "* **Chatbots and conversational AI:** Building more natural and intelligent conversational agents.\n",
      "* **Knowledge discovery and analysis:** Extracting insights and patterns from large amounts of text data.\n",
      "\n",
      "**Overall, LangGraph is a powerful tool for anyone working with natural language. Its ability to represent and reason about language in a structured and comprehensive way makes it a valuable asset for a wide range of NLP applications.**\n"
     ]
    }
   ],
   "source": [
    "res = gemini_llm.invoke(\"what is langgraph?\")\n",
    "res.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Supervisor(BaseModel):\n",
    "    \"\"\"select agent and explantion for selecting the agent\"\"\"\n",
    "\n",
    "    agent: Literal[\"ai_expert\", \"stock_trader\", \"generic\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"select following agents based user question agents: ai_expert, stock_trader, generic\"\"\",\n",
    "    )\n",
    "\n",
    "    explanation: str = Field(description=\"explain the reason for selecting specific agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            \"You are a supervisor tasked with selecting right agent to act next \n",
    "            \n",
    "            ai_expert\n",
    "            Expertise:  AI, Data science\n",
    "            Description: An ai_researcher is an expert in researching and developing new machine learning and AI models, algorithms, and techniques. They have in-depth knowledge of data science and can collect, analyze, and interpret complex data to inform their research.\n",
    "\n",
    "            stock_trader\n",
    "            Expertise: Stock market analysis, trading\n",
    "            Description: A stock_trader is an expert in analyzing and predicting stock market trends, and making informed trading decisions. They have in-depth knowledge of financial markets, economic indicators, and trading strategies.\n",
    "\n",
    "            generic\n",
    "            Expertise: General topics\n",
    "            \n",
    "            Description: If a topic does not fall under the specific expertise of the other agents, the query will default to the generic agent. This ensures that a wide array of questions can be addressed, even if they do not pertain to the specialized fields of the other agents.\n",
    "\n",
    "            specify reason for selecting the agent\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm = openai_llm.with_structured_output(Supervisor)\n",
    "\n",
    "structured_llm = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent='ai_expert' explanation=\"The user is asking about learning the Spring framework, which is a Java-based framework for building enterprise-level applications. The ai_expert agent is selected because it has expertise in AI and Data science, and learning a framework like Spring requires a strong foundation in programming and software development, which is within the ai_expert's domain.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"How to learn spring framework\"\n",
    "\n",
    "res = structured_llm.invoke({\"query\": query})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Make sure to use the tavily_search_results_json tool for information. Give explanation why you are using a tool\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Construct the Tools agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "res = agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_llm import openai_llm, groq_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import *\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import *\n",
    "\n",
    "\n",
    "class Planner(BaseModel):\n",
    "    \"\"\"create outline for answering the query\"\"\"\n",
    "\n",
    "    steps: str = Field(description=\"\"\" create outline for answering the query\"\"\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "As a planner,  create outline for answering the query\n",
    "            Previous conversation history:\n",
    "\n",
    "            {chat_history}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm = groq_llm.with_structured_output(Planner)\n",
    "\n",
    "structured_llm = prompt | structured_llm\n",
    "\n",
    "\n",
    "# # Create an agent executor by passing in the agent and tools\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Identify the motivations behind Osama bin Laden's actions\n",
      "2. Discuss the events leading up to the 9/11 attacks\n",
      "3. Explain the role of Al-Qaeda in the attacks\n",
      "4. Describe the impact of the attacks on the United States and the world\n",
      "5. Provide context on the aftermath of the attacks and the response from the international community\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"why osama bin laden destroyed twin towers \"\"\"\n",
    "response = structured_llm.invoke({\"query\": query, \"chat_history\": []})\n",
    "print(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mwhy osama bin laden destroyed twin towers \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m structured_llm\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: []})\n\u001b[0;32m----> 4\u001b[0m plan_str \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(plan_str)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "query = \"\"\"why osama bin laden destroyed twin towers \"\"\"\n",
    "response = structured_llm.invoke({\"query\": query, \"chat_history\": []})\n",
    "\n",
    "plan_str = response.steps\n",
    "print(plan_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a detailed explanation of the events and motivations behind the destruction of the Twin Towers by Osama bin Laden.\n"
     ]
    }
   ],
   "source": [
    "print(response.response_style_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Osama bin Laden motivations for 9/11 attacks'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://en.wikipedia.org/wiki/Motives_for_the_September_11_attacks', 'content': 'In an essay, Doran argued that bin Laden\\'s followers \"consider themselves an island of true believers surrounded by a sea of iniquity\".[65] Doran further argued that bin Laden hoped that US retaliation would unite the faithful against the West, sparking revolutions in Arab nations and elsewhere, and that the Osama bin Laden videos were attempting to provoke a visceral reaction in the Middle East aimed at a violent reaction by Muslim citizens to increased US involvement in their region.[66]\\nBergen argued that the attacks were part of a plan to cause the United States to increase its military and cultural presence in the Middle East, thereby forcing Muslims to confront the idea of a non-Muslim government and establish conservative Islamic governments in the region.[67]\\nU.S. President George W. Bush did in fact declare a War on Terror, which resulted in the temporary loss of control of Afghanistan by the Al-Qaeda-allied Taliban after fighting for two decades. After the end of the Gulf War and after the Iraqi withdrawal from Kuwait, the sanctions were linked to removal of weapons of mass destruction by Resolution 687.[36]\\nIn the 1998 fatwa, al-Qaeda identified the Iraq sanctions as a reason to kill Americans:\\ndespite the great devastation inflicted on the Iraqi people by the crusader-Zionist alliance, and despite the huge number of those killed, which has exceeded 1 million ... despite all this, the Americans are once again trying to repeat the horrific massacres, as though they are not content with the protracted blockade imposed after the ferocious war or the fragmentation and devastation. ... Despite this, you refuse to sign the Kyoto agreement so that you can secure the profit of your greedy companies and industries.[51]\\nAyman al-Zawahiri, said global warming reflected\\nhow brutal and greedy the Western Crusader world is, with America at its top [52]\\nBin Laden has also called for the destruction of the American economy as a way of fighting global warming.[53]\\nAmerican immorality[edit]\\nIn the above-mentioned letter, bin Laden lamented the immoral behavior that has become the norm in the United States as a motivating factor in his decision to launch the attacks:\\nThe second thing we call you to, is to stop your oppression, lies, immorality and debauchery that has spread among you. The fatwā also denounced the presence of US troops in Saudi Arabia and asserted that Americans were attempting to balkanize and destabilize the Arab world with the intent of guaranteeing \"Israel\\'s survival\":[4][5]\\n\"..for over seven years the United States has been occupying the lands of Islam in the holiest of places, the Arabian Peninsula, plundering its riches, dictating to its rulers, humiliating its people, terrorizing its neighbors, and turning its bases in the Peninsula into a spearhead through which to fight the neighboring Muslim peoples. In both of these fatāwā, bin Laden sharply criticized the financial contributions of the American government to the Saudi royal family as well as American military intervention in the Arab world.[3]\\nThese motivations were published in bin Laden\\'s November 2002 Letter to the American people,[4][5] in which he said that al-Qaeda\\'s motives for the attacks included Western support for attacking Muslims in Somalia, supporting Russian atrocities against Muslims in Chechnya, supporting the Indian oppression against Muslims in Kashmir, condoning the 1982 massacres in Lebanon, the presence of US troops in Saudi Arabia,[5][6][7]'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'events leading up to 9/11 attacks'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://en.wikipedia.org/wiki/Timeline_of_the_September_11_attacks', 'content': 'The following timeline is a chronological list of all the major events leading up to, during, and immediately following the September 11 attacks against the United States in 2001, through the first anniversary of the attacks in 2002. ... 9/11 hijacker Jarrah has unofficial wedding; photograph later suggests German intelligence has informant.'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Al-Qaeda role in 9/11 attacks'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.nbcnews.com/news/world/was-al-qaeda-leader-911-mastsermind-ayman-al-zawahiri-rcna41067', 'content': 'Called a mastermind of the Sept. 11, 2001, attacks, the leader of Al Qaeda, Ayman al-Zawahiri, has been one of the most sought-after people by the U.S. for over two decades. Al-Zawahiri, a trained ...'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'impact of 9/11 attacks on global politics'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.brookings.edu/articles/twenty-years-later-how-americans-assess-the-effects-of-the-9-11-attacks/', 'content': \"The 9/11 attacks and their aftermath have changed Americans' perceptions of our foreign policy. ... U.S. Government & Politics 20 years later: The lasting impact of 9/11 on ... and global levels ...\"}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'consequences of 9/11 attacks on United States and the world'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.kqed.org/lowdown/14066/13-years-later-four-major-lasting-impacts-of-911', 'content': 'Less than a month after 9/11, U.S. troops invaded Afghanistan in an attempt to dismantle al-Qaeda — the terrorist group that claimed responsibility for the attacks — and remove the Taliban government harboring it. Two years later, in March 2003, the United States invaded Iraq and deposed President Saddam Hussein.'}]\u001b[0m\u001b[32;1m\u001b[1;3mOsama bin Laden destroyed the Twin Towers as part of a larger plan to inflict harm on the United States and its interests. The motivations behind his actions were rooted in his extremist ideology and a desire to defend Islam against perceived Western aggression.\n",
      "\n",
      "Bin Laden was motivated by a range of factors, including:\n",
      "\n",
      "1. Opposition to US foreign policy in the Middle East, particularly the presence of US troops in Saudi Arabia.\n",
      "2. Anger over US support for Israel and its treatment of Palestinians.\n",
      "3. A desire to unite Muslims against the West and establish a global Islamic caliphate.\n",
      "\n",
      "The events leading up to the 9/11 attacks were complex and multifaceted. They included:\n",
      "\n",
      "1. The Soviet-Afghan War, which drew bin Laden and other jihadists to Afghanistan to fight against the Soviet Union.\n",
      "2. The Gulf War, which led to the presence of US troops in Saudi Arabia and further radicalized bin Laden.\n",
      "3. The rise of Al-Qaeda, which bin Laden founded in the late 1980s and which became a global network of jihadists.\n",
      "\n",
      "Al-Qaeda played a central role in the 9/11 attacks, which were planned and carried out by a group of operatives led by bin Laden. The attacks were intended to inflict maximum damage on the United States and its economy, and to create a sense of fear and uncertainty among the American people.\n",
      "\n",
      "The impact of the 9/11 attacks on global politics and international relations was significant. They led to:\n",
      "\n",
      "1. The launch of the War on Terror, a global military campaign aimed at rooting out Al-Qaeda and other jihadist groups.\n",
      "2. The invasion of Afghanistan, which toppled the Taliban regime and led to a long and bloody conflict.\n",
      "3. The invasion of Iraq, which was justified by the US government as part of the War on Terror but was widely criticized as a mistake.\n",
      "\n",
      "The consequences of the 9/11 attacks on the United States and the world were far-reaching. They included:\n",
      "\n",
      "1. The loss of thousands of lives, both on the day of the attacks and in the subsequent wars in Afghanistan and Iraq.\n",
      "2. A significant increase in military spending and a shift towards a more militarized foreign policy.\n",
      "3. A rise in Islamophobia and anti-Muslim sentiment in the United States and other Western countries.\n",
      "4. A decline in civil liberties and an increase in surveillance and security measures.\n",
      "\n",
      "Overall, the 9/11 attacks were a pivotal moment in modern history, with far-reaching consequences for global politics, international relations, and the lives of millions of people around the world.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Osama bin Laden destroyed the Twin Towers as part of a larger plan to inflict harm on the United States and its interests. The motivations behind his actions were rooted in his extremist ideology and a desire to defend Islam against perceived Western aggression.\n",
       "\n",
       "Bin Laden was motivated by a range of factors, including:\n",
       "\n",
       "1. Opposition to US foreign policy in the Middle East, particularly the presence of US troops in Saudi Arabia.\n",
       "2. Anger over US support for Israel and its treatment of Palestinians.\n",
       "3. A desire to unite Muslims against the West and establish a global Islamic caliphate.\n",
       "\n",
       "The events leading up to the 9/11 attacks were complex and multifaceted. They included:\n",
       "\n",
       "1. The Soviet-Afghan War, which drew bin Laden and other jihadists to Afghanistan to fight against the Soviet Union.\n",
       "2. The Gulf War, which led to the presence of US troops in Saudi Arabia and further radicalized bin Laden.\n",
       "3. The rise of Al-Qaeda, which bin Laden founded in the late 1980s and which became a global network of jihadists.\n",
       "\n",
       "Al-Qaeda played a central role in the 9/11 attacks, which were planned and carried out by a group of operatives led by bin Laden. The attacks were intended to inflict maximum damage on the United States and its economy, and to create a sense of fear and uncertainty among the American people.\n",
       "\n",
       "The impact of the 9/11 attacks on global politics and international relations was significant. They led to:\n",
       "\n",
       "1. The launch of the War on Terror, a global military campaign aimed at rooting out Al-Qaeda and other jihadist groups.\n",
       "2. The invasion of Afghanistan, which toppled the Taliban regime and led to a long and bloody conflict.\n",
       "3. The invasion of Iraq, which was justified by the US government as part of the War on Terror but was widely criticized as a mistake.\n",
       "\n",
       "The consequences of the 9/11 attacks on the United States and the world were far-reaching. They included:\n",
       "\n",
       "1. The loss of thousands of lives, both on the day of the attacks and in the subsequent wars in Afghanistan and Iraq.\n",
       "2. A significant increase in military spending and a shift towards a more militarized foreign policy.\n",
       "3. A rise in Islamophobia and anti-Muslim sentiment in the United States and other Western countries.\n",
       "4. A decline in civil liberties and an increase in surveillance and security measures.\n",
       "\n",
       "Overall, the 9/11 attacks were a pivotal moment in modern history, with far-reaching consequences for global politics, international relations, and the lives of millions of people around the world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = f\"\"\"\n",
    "For below user query\n",
    "{query} \n",
    "Use below points while responding: \n",
    "{plan_str}\n",
    "                      \"\"\"\n",
    "\n",
    "res = agent_executor.invoke({\"input\": input})\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown(res[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 2008 market crash, also known as the Global Financial Crisis (GFC), was a complex and multifaceted event with various contributing factors. Here's a breakdown of the key events and factors that led to the crisis:\n",
       "\n",
       "**Key Events Leading Up to the 2008 Market Crash:**\n",
       "\n",
       "1. **Housing Market Boom (2000-2006)**: The US housing market experienced a significant boom, fueled by low interest rates, lax lending standards, and excessive speculation.\n",
       "2. **Subprime Mortgage Crisis (2006-2007)**: Many subprime borrowers, who were not able to afford mortgages, defaulted on their loans, causing a surge in foreclosures and a subsequent decline in housing prices.\n",
       "3. **Financial Deregulation (1999-2004)**: The Gramm-Leach-Bliley Act (1999) repealed parts of the Glass-Steagall Act, allowing commercial banks to engage in investment activities, increasing their exposure to risk. The Securities and Exchange Commission (SEC) also relaxed regulations, enabling financial institutions to take on more leverage.\n",
       "4. **Excessive Leverage and Risk-Taking (2004-2007)**: Financial institutions, including banks and investment banks, took on excessive leverage and risk, creating complex financial instruments, such as mortgage-backed securities (MBS) and collateralized debt obligations (CDOs).\n",
       "\n",
       "**Role of Subprime Mortgage Crisis:**\n",
       "\n",
       "The subprime mortgage crisis played a significant role in the 2008 market crash. Many subprime borrowers were unable to afford their mortgages, leading to a wave of defaults and foreclosures. This caused a decline in housing prices, which in turn led to a decline in the value of mortgage-backed securities (MBS) and collateralized debt obligations (CDOs) held by financial institutions.\n",
       "\n",
       "**Impact of Housing Market Bubble Burst:**\n",
       "\n",
       "The housing market bubble burst in 2006-2007, leading to a sharp decline in housing prices. This decline in housing prices reduced the value of mortgage-backed securities (MBS) and collateralized debt obligations (CDOs), causing significant losses for financial institutions.\n",
       "\n",
       "**Effect of Financial Deregulation:**\n",
       "\n",
       "Financial deregulation contributed to the 2008 market crash by allowing financial institutions to take on excessive leverage and risk. The Gramm-Leach-Bliley Act (1999) and the SEC's relaxation of regulations enabled commercial banks to engage in investment activities, increasing their exposure to risk.\n",
       "\n",
       "**Consequences of Excessive Leverage and Risk-Taking:**\n",
       "\n",
       "Excessive leverage and risk-taking by financial institutions led to a significant increase in their exposure to risk. When the housing market bubble burst, many financial institutions found themselves with large amounts of worthless mortgage-backed securities (MBS) and collateralized debt obligations (CDOs), leading to significant losses and eventual collapse.\n",
       "\n",
       "**Timeline of Major Events During the Crisis:**\n",
       "\n",
       "* August 2007: French bank BNP Paribas halts withdrawals from its investment funds, citing subprime mortgage exposure.\n",
       "* March 2008: Bear Stearns, a US investment bank, is sold to JPMorgan Chase after facing financial difficulties.\n",
       "* September 7, 2008: Fannie Mae and Freddie Mac, US government-sponsored mortgage giants, are placed into conservatorship.\n",
       "* September 14, 2008: Merrill Lynch, a US investment bank, is sold to Bank of America.\n",
       "* September 15, 2008: Lehman Brothers, a US investment bank, files for bankruptcy.\n",
       "* October 3, 2008: The US government passes the Troubled Asset Relief Program (TARP) to provide $700 billion in bailout funds to struggling financial institutions.\n",
       "\n",
       "**Key Factors That Contributed to the 2008 Market Crash:**\n",
       "\n",
       "1. **Subprime Mortgage Crisis**: Widespread defaults on subprime mortgages led to a decline in housing prices and a subsequent decline in the value of mortgage-backed securities (MBS) and collateralized debt obligations (CDOs).\n",
       "2. **Financial Deregulation**: The Gramm-Leach-Bliley Act (1999) and the SEC's relaxation of regulations enabled financial institutions to take on excessive leverage and risk.\n",
       "3. **Excessive Leverage and Risk-Taking**: Financial institutions took on excessive leverage and risk, creating complex financial instruments that ultimately led to significant losses.\n",
       "4. **Housing Market Bubble Burst**: The housing market bubble burst in 2006-2007, leading to a sharp decline in housing prices and a subsequent decline in the value of mortgage-backed securities (MBS) and collateralized debt obligations (CDOs).\n",
       "\n",
       "In summary, the 2008 market crash was caused by a combination of factors, including the subprime mortgage crisis, financial deregulation, excessive leverage and risk-taking, and the housing market bubble burst. These factors created a perfect storm that ultimately led to the collapse of several major financial institutions and a global economic downturn."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = groq_llm.invoke(\n",
    "    f\"\"\"\n",
    "For below user query\n",
    "{query} \n",
    "Use below points while responding: \n",
    "{plan_str}\n",
    "\n",
    "                      \"\"\"\n",
    ")\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "\n",
    "def get_results_from_tavily(search_queries):\n",
    "    queries_docs = tavily_retriever.batch(search_queries)\n",
    "    docs = [{\"href\": doc.metadata[\"source\"], \"content\": doc.page_content} for docs in queries_docs for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(search_queries: str, explanation: str) -> str:\n",
    "    \"\"\"The search_web tool allows to augment knowledge and retrieve the latest information on a topic by searching the web using provided comma-separated search queries max queries 2. This tool returns a summary of the key points from all retrieved web pages, enabling me to provide more accurate and up-to-date information.\n",
    "\n",
    "    Args:\n",
    "        search_queries (str): comma seperated search queries without quotes, related to the topic, max queries 3\n",
    "        explanation (str): give a reason for choosing this tool\n",
    "\n",
    "    Returns:\n",
    "        str: summary of all web pages retrieved for a search query\n",
    "    \"\"\"\n",
    "\n",
    "    search_queries = search_queries.split(\",\")\n",
    "    # search_queries = [clean_text(query) for query in search_queries]\n",
    "\n",
    "    urls = []\n",
    "    results = get_results_from_tavily(search_queries)\n",
    "    urls += [item[\"href\"] for item in results]\n",
    "\n",
    "    print(\"****   WEB SEARCH  *****\")\n",
    "    print(search_queries)\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "\n",
    "    summaries = \"\\n\\n\".join([f\"\\n\\nContent: {(doc['content'])}\" for idx, doc in enumerate(results)])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****   WEB SEARCH  *****\n",
      "['elon musk wikipedia', ' jeff bezos']\n",
      "https://www.wikiwand.com/en/Elon_Musk\n",
      "https://simple.wikipedia.org/wiki/Elon_Musk\n",
      "https://www.britannica.com/money/Jeff-Bezos\n",
      "https://en.wikipedia.org/wiki/Jeff_Bezos\n",
      "\n",
      "\n",
      "Content: He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023[update], according to the Bloomberg Billionaires Index, and $254 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.[5][6]\n",
      "A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. He is the founder, chairman, CEO, and chief technology officer of SpaceX; angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. Elon Musk\n",
      "Businessman (born 1971) /\n",
      "From Wikipedia, the free\n",
      "encyclopedia\n",
      "Can you list the top facts and stats about Elon Musk?\n",
      "Summarize this article for a 10 year old\n",
      "Elon Reeve Musk (/ˈiːlɒn/ EE-lon; born June 28, 1971) is a businessman and investor. His ownership of Twitter has been similarly controversial, including; laying off a large number of employees, an increase in hate speech and misinformation and disinformation on the website, as well as changes to Twitter Blue verification. In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company.\n",
      "\n",
      "\n",
      "\n",
      "Content: In 2022, he became the owner of the social media site Twitter after buying it for USD $44 billion.[2]\n",
      "Musk is the second-wealthiest person in the world, according to both the Bloomberg Billionaires Index and Forbes's real-time billionaires list.[3][4] These were to start businesses in the internet, renewable energy and outer space.[10] He became an American citizen in 2002.[11]\n",
      "Projects[change | change source]\n",
      "At the end of 2016, Musk founded The Boring Company which focuses on tunneling and infrastructure. Musk has Asperger's which is a former type of Autism.[5]\n",
      "Early life and career[change | change source]\n",
      "Elon Musk was born to a Canadian mother and a South African-born British father in Pretoria, South Africa on June 28, 1971.[6] Then he moved to California to attempt to get a PhD in applied physics at Stanford but he left the program after only two days because he wanted to pursue his entrepreneurial aspirations. He is also the CEO of Solar City, a company that makes solar panels, and the CEO & CTO of SpaceX, an aerospace company.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Content: Jeff Bezos (born January 12, 1964, Albuquerque, New Mexico, U.S.) is an American entrepreneur who played a key role in the growth of e-commerce as the founder and chief executive officer of Amazon.com, Inc., an online merchant of books and later of a wide variety of products.\n",
      "\n",
      "\n",
      "\n",
      "Content: He warned many early investors that there was a 70% chance that Amazon would fail or go bankrupt.[53] Although Amazon was originally an online bookstore, Bezos had always planned to expand to other products.[43][49] Three years after Bezos founded Amazon, he took it public with an initial public offering (IPO).[54] In response to critical reports from Fortune and Barron's, Bezos maintained that the growth of the Internet would overtake competition from larger book retailers such as Borders and Barnes & Noble.[49]\n",
      "In 1998, Bezos diversified into the online sale of music and video, and by the end of the year he had expanded the company's products to include a variety of other consumer goods.[49] Bezos used the $54 million raised during the company's 1997 equity offering to finance the aggressive acquisition of smaller competitors.[49] In 2000, Bezos borrowed $2 billion from banks, as its cash balances dipped to only $350 million.[55] In January 2019, Bezos and his wife Mackenzie released a joint statement which revealed that they would be getting divorced after 25 years together.[250][251][252] Subsequently, National Enquirer revealed that Bezos had an affair with media personality Lauren Sánchez; the affair with Sánchez had lasted for months.[253][254] Later, Bezos published an online essay on February 7, 2019, in which he accused American Media, Inc. owner David Pecker of \"extortion and blackmail\" for threatening to publish intimate photos of Bezos and current girlfriend Lauren Sánchez[255] if he did not stop his investigation into how his text messages and other photos had been leaked to the National Enquirer.[256][257] Media reports have accused Sánchez's brother Michael of being the source for the photos obtained by National Enquirer; however, Bezos has speculated that it may have been the Saudi Arabian government.[258]\n",
      "On April 4, 2019, the divorce was finalized, with Bezos keeping 75% of the couple's Amazon stock and MacKenzie getting the remaining 25% ($35.6 billion) in Amazon stock. \"[275] Despite Bezos's support for an open borders policy towards immigrants, Amazon has actively marketed facial recognition software to U.S. Immigration and Customs Enforcement (ICE).[276]\n",
      "In 2019, a political action committee linked to Bezos spent over $1 million in an unsuccessful attempt to defeat the reelection bid of Seattle city council member and activist Kshama Sawant.[277]\n",
      "On November 22, 2021, Jeff Bezos donated $100 million to the Obama Foundation to \"help expand the scope of programming that reaches emerging leaders\", and requested the Obama Presidential Center's plaza to be named after John Lewis.[278][279]\n",
      "Saudi hacking claim\n",
      "In March 2018, Bezos met in Seattle with Mohammad bin Salman, the crown prince and de facto ruler of Saudi Arabia, to discuss investment opportunities for Saudi Vision 2030.[280] In March 2019, Bezos's security consultant accused the Saudi government of hacking Bezos's phone. With Bezos at the helm and Scott taking an integral role in its operation—writing checks, keeping track of the books, and negotiating the company's first freight contracts—the foundation was laid for this garage-run operation to grow exponentially.[47] Prior to settling in Seattle, Bezos had investigated setting up his company at an Indian reservation near San Francisco in order to avoid paying taxes.[48] Bezos initially named his new company Cadabra but later changed the name to Amazon after the Amazon River in South America, in part because the name begins with the letter A, which is at the beginning of the alphabet.[49] At the time, website listings were alphabetized, so a name starting with \"A\" would appear sooner when customers conducted online searches.[50] At the time of the post, Bezos's lifetime spending on charitable causes was estimated to be $100 million.[26] Multiple opinion columnists responded by asking Bezos to pay higher wages to Amazon warehouse workers.[291][292] A year later in June, he tweeted that he would announce two philanthropic foci by the end of summer 2018.[293] Bezos announced in September 2018 that he would commit approximately $2 billion to a fund to deal with American homelessness and establish a network of non-profit preschools for low income communities.[294] As part of this announcement, he committed to establishing the \"Day 1 Families Fund\" to finance \"night shelters and day care centers for homeless families\" and the \"Day 1 Academies Fund\" for early childhood education.[295][296]\n",
      "In January 2018, Bezos made a $33 million donation to TheDream.\n"
     ]
    }
   ],
   "source": [
    "print(search_web(\"elon musk wikipedia, jeff bezos\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_graph import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724254858.566688  907556 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "for key in st.secrets.keys():\n",
    "    os.environ[key] = st.secrets[key]\n",
    "\n",
    "from load_llm import openai_llm, groq_llm, llm, gemini_llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# llm = gemini_llm\n",
    "\n",
    "\n",
    "from chat_graph import graph\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "\n",
    "def display_response(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Planner *****\n",
      "1. To explain how Large Language Models (LLM) work, I'll break down the process into a series of steps. Here's a detailed outline:\n",
      "\n",
      "**Step 1: Data Collection**\n",
      "\n",
      "* Identify a massive dataset of text from various sources, such as books, articles, research papers, and websites.\n",
      "* The dataset should be diverse, covering a wide range of topics, styles, and genres.\n",
      "* The data is typically collected and preprocessed by a team of experts to ensure its quality and relevance.\n",
      "\n",
      "**Step 2: Data Preprocessing**\n",
      "\n",
      "* Clean and preprocess the collected data by removing unnecessary characters, punctuation, and special symbols.\n",
      "* Tokenize the text into individual words or subwords (smaller units of words).\n",
      "* Convert the text into a numerical representation using techniques such as word embeddings (e.g., Word2Vec, GloVe) or one-hot encoding.\n",
      "\n",
      "**Step 3: Model Architecture**\n",
      "\n",
      "* Design a neural network architecture that can process and learn from the preprocessed data.\n",
      "* Common architectures for LLMs include:\n",
      "\t+ Recurrent Neural Networks (RNNs)\n",
      "\t+ Long Short-Term Memory (LSTM) networks\n",
      "\t+ Transformers\n",
      "* The architecture should be able to handle the massive size of the dataset and learn complex patterns and relationships.\n",
      "\n",
      "**Step 4: Model Training**\n",
      "\n",
      "* Train the model on the preprocessed dataset using a suitable optimization algorithm (e.g., stochastic gradient descent, Adam).\n",
      "* The model learns to predict the next word or token in a sequence, given the context of the previous words.\n",
      "* The training process involves multiple iterations, with the model adjusting its parameters to minimize the loss function.\n",
      "\n",
      "**Step 5: Model Evaluation**\n",
      "\n",
      "* Evaluate the trained model on a separate test dataset to assess its performance.\n",
      "* Metrics such as perplexity, accuracy, and F1-score are used to evaluate the model's ability to predict the next word or token.\n",
      "* The model's performance is also evaluated on specific tasks, such as language translation, sentiment analysis, or question answering.\n",
      "\n",
      "**Step 6: Model Fine-Tuning**\n",
      "\n",
      "* Fine-tune the pre-trained model on a specific task or domain to adapt it to the task requirements.\n",
      "* This involves retraining the model on a smaller dataset related to the task, while keeping the pre-trained weights as a starting point.\n",
      "* Fine-tuning allows the model to learn task-specific patterns and relationships.\n",
      "\n",
      "**Step 7: Model Deployment**\n",
      "\n",
      "* Deploy the fine-tuned model in a production environment, where it can be used to generate text, answer questions, or perform other tasks.\n",
      "* The model can be integrated with other systems, such as chatbots, virtual assistants, or content generation platforms.\n",
      "\n",
      "**Step 8: Model Maintenance and Updates**\n",
      "\n",
      "* Continuously monitor the model's performance and update it as needed to maintain its accuracy and relevance.\n",
      "* New data can be added to the model to keep it up-to-date with the latest developments and trends.\n",
      "* The model can be retrained or fine-tuned to adapt to changes in the task requirements or domain.\n",
      "\n",
      "By following these steps, LLMs can be developed and deployed to perform a wide range of tasks, from language translation and text summarization to question answering and content generation.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": int(time.time())}}\n",
    "\n",
    "prompt = \" explain how LLM works \"\n",
    "response = graph.invoke({\"messages\": [(\"user\", prompt)]}, config)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**How Large Language Models (LLMs) Work: A Step-by-Step Explanation**\n",
       "\n",
       "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling machines to understand, generate, and interact with human language. To explain how LLMs work, I'll break down the process into a series of steps, as outlined below.\n",
       "\n",
       "**Step 1: Data Collection**\n",
       "\n",
       "The first step in building an LLM is to collect a massive dataset of text from various sources, such as books, articles, research papers, and websites. This dataset should be diverse, covering a wide range of topics, styles, and genres. The data is typically collected and preprocessed by a team of experts to ensure its quality and relevance.\n",
       "\n",
       "According to a study by Radford et al. (2019), the dataset used to train the popular language model BERT consists of 3.3 billion parameters and 16 GB of text data [1]. The dataset includes a mix of books, articles, and websites, which helps the model learn to recognize patterns and relationships in language.\n",
       "\n",
       "**Step 2: Data Preprocessing**\n",
       "\n",
       "Once the dataset is collected, it needs to be cleaned and preprocessed to remove unnecessary characters, punctuation, and special symbols. This step is crucial in preparing the data for the model to learn from.\n",
       "\n",
       "Tokenization is another important step in data preprocessing, where the text is broken down into individual words or subwords (smaller units of words). This helps the model to understand the relationships between words and learn to predict the next word in a sequence.\n",
       "\n",
       "Word embeddings, such as Word2Vec and GloVe, are used to convert the text into a numerical representation. This allows the model to learn the semantic meaning of words and their relationships with each other.\n",
       "\n",
       "**Step 3: Model Architecture**\n",
       "\n",
       "The next step is to design a neural network architecture that can process and learn from the preprocessed data. Common architectures for LLMs include Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers.\n",
       "\n",
       "RNNs are well-suited for sequential data, such as text, and can learn to recognize patterns and relationships in language. LSTMs are a type of RNN that can handle long-term dependencies in data.\n",
       "\n",
       "Transformers, on the other hand, are a type of neural network that uses self-attention mechanisms to process sequential data. They have been shown to be highly effective in language modeling tasks and are widely used in LLMs.\n",
       "\n",
       "**Step 4: Model Training**\n",
       "\n",
       "Once the model architecture is designed, it needs to be trained on the preprocessed dataset using a suitable optimization algorithm, such as stochastic gradient descent or Adam.\n",
       "\n",
       "The model learns to predict the next word or token in a sequence, given the context of the previous words. This is done by minimizing a loss function, such as cross-entropy or mean squared error.\n",
       "\n",
       "The training process involves multiple iterations, with the model adjusting its parameters to minimize the loss function. This process can take several days or even weeks, depending on the size of the dataset and the complexity of the model.\n",
       "\n",
       "**Step 5: Model Evaluation**\n",
       "\n",
       "After the model is trained, it needs to be evaluated on a separate test dataset to assess its performance. Metrics such as perplexity, accuracy, and F1-score are used to evaluate the model's ability to predict the next word or token.\n",
       "\n",
       "Perplexity is a measure of how well the model predicts the next word in a sequence. A lower perplexity score indicates better performance.\n",
       "\n",
       "Accuracy and F1-score are measures of how well the model performs on specific tasks, such as language translation or sentiment analysis.\n",
       "\n",
       "**Step 6: Model Fine-Tuning**\n",
       "\n",
       "Once the model is trained and evaluated, it can be fine-tuned on a specific task or domain to adapt it to the task requirements. This involves retraining the model on a smaller dataset related to the task, while keeping the pre-trained weights as a starting point.\n",
       "\n",
       "Fine-tuning allows the model to learn task-specific patterns and relationships, which can improve its performance on the task.\n",
       "\n",
       "**Step 7: Model Deployment**\n",
       "\n",
       "After fine-tuning, the model can be deployed in a production environment, where it can be used to generate text, answer questions, or perform other tasks.\n",
       "\n",
       "The model can be integrated with other systems, such as chatbots, virtual assistants, or content generation platforms.\n",
       "\n",
       "**Step 8: Model Maintenance and Updates**\n",
       "\n",
       "Finally, the model needs to be continuously monitored and updated to maintain its accuracy and relevance. New data can be added to the model to keep it up-to-date with the latest developments and trends.\n",
       "\n",
       "The model can be retrained or fine-tuned to adapt to changes in the task requirements or domain.\n",
       "\n",
       "In conclusion, LLMs are complex systems that require careful design, training, and evaluation to achieve good performance. By following the steps outlined above, LLMs can be developed and deployed to perform a wide range of tasks, from language translation and text summarization to question answering and content generation.\n",
       "\n",
       "References:\n",
       "\n",
       "[1] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2019). Improving Language Understanding by Generative Models. arXiv preprint arXiv:1907.07655.\n",
       "\n",
       "[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1908.08504.\n",
       "\n",
       "[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762.\n",
       "\n",
       "[4] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\"configurable\": {\"thread_id\": int(time.time())}}\n",
    "\n",
    "prompt = \" generate cover story for upwork with experience in deep learning\"\n",
    "response = graph.stream({\"messages\": [(\"user\", prompt)]}, stream_mode=\"values\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " generate cover story for upwork with experience in deep learning\n",
      "***** Planner *****\n",
      "1. 1. Identify relevant work experience in deep learning, 2. Highlight key skills and accomplishments, 3. Emphasize transferable skills, 4. Create a compelling personal statement, 5. Develop a professional profile, 6. Prepare a portfolio of work, 7. Research and tailor the proposal to the client, 8. Write a clear and concise proposal, 9. Include relevant certifications and education, 10. Edit and proofread the proposal for grammar and spelling errors\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " generate cover story for upwork with experience in deep learning\n",
      "****   WEB SEARCH  *****\n",
      "['upwork cover letter deep learning experience']\n",
      "https://community.upwork.com/t5/Freelancers/Crafting-the-Perfect-Upwork-Cover-Letter-A-Comprehensive-Guide/m-p/1653151\n",
      "https://ebizfacts.com/upwork-cover-letter/\n",
      "https://gizmowatch.com/cover-letter-upwork-sample/\n",
      "https://www.vollna.com/blog/how-to-write-a-cover-letter-basics-and-examples-for-upwork-success\n",
      "https://workpajama.com/upwork-cover-letter/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 5/5 [00:01<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided content and steps, here's a sample cover letter for Upwork with experience in deep learning:\n",
      "\n",
      "Dear [Client's Name],\n",
      "\n",
      "I'm excited to apply for your deep learning project. With over 5 years of experience in deep learning, I have the skills and expertise to deliver high-quality results.\n",
      "\n",
      "**Skills Highlight:**\n",
      "\n",
      "* Proficient in TensorFlow and PyTorch\n",
      "* Experience with convolutional neural networks (CNNs) and recurrent neural networks (RNNs)\n",
      "* Strong understanding of transfer learning and fine-tuning pre-trained models\n",
      "\n",
      "**Relevant Experience:**\n",
      "\n",
      "* Developed a deep learning model for image classification that achieved an accuracy of 95% on the CIFAR-10 dataset.\n",
      "* Built a natural language processing (NLP) model that improved sentiment analysis by 20% on a benchmark dataset.\n",
      "\n",
      "**Call to Action:**\n",
      "\n",
      "I would love to discuss your project in more detail. Please feel free to reach out to schedule a call.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "This cover letter highlights your relevant experience and skills in deep learning, and includes a clear call to action to encourage the client to contact you for further discussion.\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
